---
title             : "Using Beta Regression to Model Normative Aspects of Prejudice and Political Attitudes"
shorttitle        : "Beta Regression"

author: 
  - name          : "Mark H. White II"
    affiliation   : "1"
    corresponding : yes
    email         : "markhwhiteii@gmail.com"

affiliation:
  - id            : "1"
    institution   : "University of Kansas"

author_note: >
  I would like to thank Dr. Achim Zeileis and Dr. Jonathan Templin for comments on an earlier version of this manuscript.

abstract: >
  Abstract will go here.
  
keywords          : "beta regression, hurdle models, gamlss, norms, social attitudes"

bibliography      : ["r-references.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no

lang              : "english"
class             : "man"
output            : papaja::apa6_pdf
---

```{r message = FALSE, warning = FALSE, error = FALSE, echo = FALSE}
library(tidyverse)
library(knitr)
```

# Statistical Background

## The Beta Distribution

The beta distribution can be used in a generalized linear model when the values of the dependent variable are bounded $0 < y < 1$ [@coxe2013generalized]. The probability density function (pdf) of the beta distribution is determined by two parameters, $\alpha$ and $\beta$, that are called "shape" parameters:  

\begin{center}
$f(y;\alpha,\beta)={\Gamma(\alpha+\beta)\over\Gamma(\alpha)\Gamma(\beta)}y^{\alpha-1}(1-y)^{\beta-1}$
\end{center}

where $\Gamma(.)$ is the gamma function. One of the benefits of the beta distribution is that it is flexible and can take a number of distributional shapes (Figure 1).  

```{r echo = FALSE, fig.cap = "Beta probability density functions with various combinations of shape parameters."}
shapes <- expand.grid(c(1.0, 5.0), c(1.0, 5.0))
names(shapes) <- c("shape1", "shape2")
shapes <- rbind(shapes, data.frame(shape1 = 0.8, shape2 = 0.8))
x_ <- seq(.001, .999, length = 500)
x <- c()
p <- c()
shape1 <- c()
shape2 <- c()
for (i in 1:nrow(shapes)) {
  x <- append(x, x_)
  p <- append(p, dbeta(x_, shapes[i, 1], shapes[i, 2]))
  shape1 <- append(shape1, rep(shapes[i, 1], length(x_)))
  shape2 <- append(shape2, rep(shapes[i, 2], length(x_)))
}
dat <- data.frame(x, p, shape1 = shape1, shape2 = shape2)
dat$shapes <- as.factor(paste0("a = ", dat$shape1, ", b = ", dat$shape2))
ggplot(dat, aes(x = x, y = p, linetype = shapes)) +
  geom_line() +
  theme_minimal() +
  theme(legend.position = c(.5, .75), text = element_text(size = 16),
        legend.title = element_blank(), axis.title.x = element_blank(),
        axis.title.y = element_blank())
```

These parameters are not inherently meaningful to researchers, however. @rigby2017distributions "reparameterized" the beta distribution so that the two parameters determining the shape of the distribution would be more useful in a regression framework [but see @ferrari2004beta for a different parameterization]. Instead of predicting $\alpha$ and $\beta$, they parameterize (i.e., algebraically rearrange parameters) so that beta regression predicts two different parameters: $\mu$ (called the "location" parameter) and $\sigma$ (called the "scale" parameter), where $\mu={\alpha\over(\alpha+\beta)}$ and $\sigma=\sqrt{1\over(\alpha+\beta+1)}$. $\mu$ is equivalent to the mean, and $\sigma$ is related positively to the variance (note that $\sigma$ is *not* the standard deviation, even though $\sigma$ commonly refers to the standard deviation). The variance is equivalent to $\sigma^2\mu(1-\mu)$, and there are two things to note from this equation: First, the greater the $\sigma$, the greater the variance; second, the variance depends on the mean, which means that beta regression will be naturally heteroskedastic (covered shortly). Using this distribution in a regression framework cannot handle the dependent variable taking on values 0 and 1—how can we model dependent variables in the range $0 \leq y \leq 1$?  

## Zero-One Inflated Beta Distribution

@rigby2017distributions show that the beta distribution can be "inflated" at 0 and 1, meaning the dependent variable contains 0s and 1s (i.e., $0 \leq y \leq 1$), the pdf for this beta inflated distribution, $\text{beinf}$, is:  

\begin{center}
\[
\text{beinf}(y;\mu,\sigma,\nu,\tau) =
\begin{cases}
  p_0                             & \text{if } y = 0\\
  (1 - p_0 - p_1)f(y;\mu,\sigma)  & \text{if } 0 < y < 1\\
  p_1                             & \text{if } y = 1
\end{cases}
\]
\end{center}

for $0 \leq y \leq 1$, where $f(y;\mu,\sigma)$ is the beta pdf with $\mu$ and $\sigma$ bounded *between* zero and one. The two additional parameters, $\nu$ and $\tau$, are related to $p_0$ and $p_1$, respectively. $p_0$ is the probability that a case equals zero, $p_1$ is the probability that a case equals one, and $p_2$ (i.e., $1 - p_0 - p_1$) is the probability that the case comes from the beta distribution. In terms of these two additional parameters, $p_0 = {\nu \over (1 + \nu + \tau)}$ and $p_1 = {\tau \over (1 + \nu + \tau)}$. Rearranging these algebraically, $\nu$ is the odds that a case is 0 to being $0 < y < 1$, $\nu = p_0 / p_2$, and $\tau$ is the odds that a case is a 1 to being $0 < y < 1$, $\tau = p_1 / p_2$. It should be noted, however, that this distribution only includes *one* source of 0 values (i.e., $p_0$). For this reason, some would not call it an "inflated" distribution—which has more than one source of 0 values—but instead a "hurdle model" [discussed below, see @coxe2013generalized].  

## Beta Regression Models

The goal for a researcher now is to predict these four parameters, $\mu, \sigma, \nu, \tau$, from any number of predictor variables of theoretical interest. All four parameters can be predicted by an identical set of predictors, none of the same predictors, or any mixture. Both $\mu$ and $\sigma$ have to be between zero and one (since the beta distribution is bounded between 0 and 1), so one can use the logistic link function to fit predicted values in this range. Both $\nu$ and $\tau$ have to be greater than zero (since they are odds), so one can use the log link function to fit predicted values in this range. Imagine a researcher has one predictor of interest, $x$. They could include this variable as a predictor of all four variables with the equations:  

\begin{center}
$\text{log}({\mu \over 1 - \mu}) = \beta_{10} + \beta_{11}X$\\
$\text{log}({\sigma \over 1 - \sigma}) = \beta_{20} + \beta_{21}X$\\
$\text{log}(\nu) = \beta_{30} + \beta_{31}X$\\
$\text{log}(\tau) = \beta_{40} + \beta_{41}X$
\end{center}

Or equivalently:  

\begin{center}
$\mu={1\over1+e^{-(\beta_{10}+\beta_{11}X)}}$\\
$\sigma={1\over1+e^{-(\beta_{20}+\beta_{21}X)}}$\\
$\nu = e^{\beta_{30} + \beta_{31}X}$\\
$\tau = e^{\beta_{40} + \beta_{41}X}$
\end{center}

where $e^x$ is the natural exponential function. A different inflated beta distribution can also be used when the dependent variable contains 0s but not 1s (e.g., $0 \leq y < 1$) or when it contains 1s but not 0s (e.g., $0 < y \leq 1$). Let $c$ be the value—0 or 1—that is included (i.e., inflated). The pdf is:  

\begin{center}
\[
\text{beinf}_c(y;\mu,\sigma,\nu) =
\begin{cases}
  p_c                             & \text{if } y = c\\
  (1 - p_c)f(y;\mu,\sigma)        & \text{if } 0 < y < 1\\
\end{cases}
\]
\end{center}

where $\nu = p_c / (1 - p_c)$ and the remaining notation is the same as in the zero-and-one inflated beta distribution. The same link functions are used, but the fourth parameter, $\tau$, is not included, since the distribution is only inflated at $c$. Lastly, if no 0s or 1s are observed, the beta distribution alone can be used as the pdf. This results in a beta regression where the researcher is only predicting the location, $\mu$, and shape, $\sigma$. Since there is no inflation at 0 or 1, the latter two parameters, $\nu$ and $\tau$, are not included.  

## Modeling Bounded Variables Beyond the Zero Through One Range

These beta regression models are commonly used when rates and proportions are dependent variables, given that these are naturally bounded $0 \leq y \leq 1$ [@buntaine2011does; @eskelson2011estimating; @gallardo2012analysis; @hubben2008societal; @peplonska2012rotating]. More generally, however, the beta distribution is a doubly bounded continuous distribution: Although it is on a continuous scale, values cannot be exceed the upper bound, $u$, or be less than the lower bound, $l$. In the case of the zero-and-one inflated beta distribution, $l = 0$ and $u = 1$.  

Most scales used to measure prejudice and political attitudes are doubly bounded and continuous. Although researchers generally model variables on Likert scales and sliding scales (e.g., feeling thermometers) as being conditionally normally distributed (i.e., using ordinary least squares regression), these variables are not strictly normal. Observations from a normal distribution can take on any value on the real number line, while observations from a standard 7-point Likert scale can only take on values 1 through 7—and in studying controversial and polarizing issues like prejudice and politics, many participants score at the lower or upper bounds, causing floor or ceiling effects. In these cases, the assumptions of normality and homoskedasticity are likely to be violated. Many times researchers create, update, or seek out measures to so that the distributional form of the dependent variable takes on a normal shape—statistical assumptions are directing the phenomenon researchers choose to observe and the theoretical questions that they ask. Instead, one can explicitly take into account that the response is doubly bounded and heteroskedastic by using the beta regression models described above, using a simple linear rescaling of the data.  

If one observes a dependent variable limited between two bounds, there is a straightforward way to rescale the variable to the $0 \leq y \leq 1$ range:  

\begin{center}
$y'_i = (y_i - l) / (u - l)$
\end{center}

where $y$ is the variable on the original scale, $y'$ is the rescaled variable, $u$ is the upper bound (i.e., the largest possible value on the scale), $l$ is the lower bound (i.e., the smallest possible value on the scale), and the $i$ subscript denotes an individual's score. On a standard 7-point Likert scale, $l = 1$ and $u = 7$. This rescaling allows a researcher to explicitly model conditional variance, floor effects, and ceiling effects using beta regression.  

### Conditional variance

Observing a dependent variable that is doubly bounded and continuous can produce heteroskedasticity. One of the assumptions of an OLS regression is homoskedasticity—that the variance of the errors are unrelated to any predictor, any linear combination of predictors, and the predicted values. A regression equation with one predictor variable $x$ is often written as $y_i = \beta_0 + \beta_1x_i + \epsilon_i$, where $\epsilon_i$ is how far an osbserved value $y_i$ is from the predicted value (i.e., the residual). Let $\hat{y_i}$ be the predicted value for observation $i$, where $\hat{y_i} = \beta_0 + \beta_1x_i$. We can simplify the equation to $y_i = \hat{y_i} + \epsilon_i$, where the residuals $\epsilon$ are normally distributed with a mean of 0 and some variance—that is, $\epsilon \sim N(0, \sigma^2_\epsilon)$. We can further simplify this to: $y_i|x_i \sim N(\hat{y_i}, \sigma^2_\epsilon)$, which means that each observation of the dependent variable we make, given the predictors we use, is normally distributed with a mean of that observation's predicted score and some variance. The assumption of homoskedasticity is found in that $\sigma^2_\epsilon$ does *not* have a subscript $i$. This means that there is one *common variance* of the residuals.  

Imagine one runs a regression and observes $\hat{y_i} = 2.5 + 1.5 \times x_i$ and $\sigma^2_\epsilon = 9$. Say that the first individual's score on $x$ is 0, $x_1 = 0$, and the second individual's score is $x_2 = 5$. The model then assumes that the first person comes from a normal distribution with a mean of 4 (i.e., $\hat{y_1}$) and a variance of 9 (i.e., $\sigma^2_\epsilon$), while the second person comes from a normal distribution with a mean of 10 and that same variance of 9. The violation of this assumption can lead to inflated Type I errors or diminished statistical power, depending on the type and source of the heteroskedasticity [@hayes2007using; @long2000using; @rosopa2013managing].  

While common remedies for heteroskedasticity (e.g., robust standard errors, transforming the dependent variable, weighted least squares) treat it as a nuisance to be corrected for when calculating $p$-values [@hayes2007using; @long2000using; @rosopa2013managing], it could be that explicitly modeling this conditional variance is an interesting phenomenon per se. Heteroskedasticity can arise when the error variance is predicted by one of the observed predictor variables, and since beta regression models predict a shape parameter, $\sigma$, a researcher can explicitly model this relationship. This allows researchers to ask questions about conditional variance: "Does the variance in $y$ increase as $x$ also increases?" Since the focus of OLS regression is the *location* parameter (e.g., conditional mean), researchers might be ignoring interesting, potentially theoretically-relevant parts of their data. After discussing floor and ceiling effects, I will argue that normative influences are one of these aspects.  

### Floor and ceiling effects

Floor and ceiling effects occur when there are many observations of the dependent variable at the lower and upper bound of the scale, respectively [@everitt2002cambridge]. Variables displaying these effects are likely to violate numerous assumptions of ordinary least squares regression, such as conditional normality and homoskedasticity of variance, and can be modeled as censored or truncated. Censoring occurs when all scores beyond some threshold are recorded at that threshold. For example, if a researcher is measuring reaction times, censoring occurs if they decide that any times above 5 seconds are scored as 5. If many people took longer than 5 seconds, this censoring can cause a ceiling effect. Truncating occurs when people who score beyond some treshold are excluded from the data. If the researcher simply does not record trials that take longer than 5 seconds, then the data are truncated. A common regression technique for censored or truncated variables are Tobit models [@mcbee2010modeling; @smithson2013generalized]. However, standard Tobit models assume that the underlying latent construct is normally distributed.  

In the realm of prejudice and politics, I argue that it is *not* useful to think of prejudice as coming from a latent normal distribution. I find it unlikely that if, instead of a Likert scales going from 1 to 7, it extended from -7 to +7, one would observe a normal distribution for the Attitude Towards Blacks scale, for example [@brigham1993college]. I find it likely that participants would opt for the new floor—the negative 7. This can be thought of as a decision-making process. Using prejudice as an example, people decide whether or not they are going to admit to feeling any prejudice. If they decide not to, they simply circle or click all 1s (i.e., the floor). If they decide to admit prejudice, they respond to the items. As another example: When offering one's attitudes toward a polarizing political figure, a participant decides whether or not to respond entirely in the negative (i.e., at the floor), entirely in the positive (i.e., at the ceiling), or somewhere in between. This could result in a bimodal distribution, with both ceiling and floor effects.  

This decision making process can be modeled with the inflated beta regression models described above. Inflations at zero, one, or zero-and-one allow researchers to measuring ceiling effects, floor effects, and both simultaneously. While methodologists in beta regression refer to these models as "inflated" models, a more general label is "two-part" models [@coxe2013generalized]. When one assumes a decision-making process behind the two parts, these models are often referred to as "hurdle" models in the econometrics literature [@cameron2005microeconometrics; @wooldridge2010econometric].  

@cragg1971some demonstrated that hurdle models are a generalization of the Type I Tobit model (but note that, unlike the standard Tobit model, the latent dependent construct is assumed to be conditionally *beta* distributed, which can take on many distributional forms; see Figure 1). In economics, hurdle models are often used when dependent variables are counts with excess 0s [see @carlevaro2016multiple for a review of applications]. The log likelihood function for these models can be separated in two parts: First, a binary regression (e.g., logistic or probit) modeling the probability of observing $y = 0$ versus $y > 0$; second, a truncated negative binomial model using just the cases where $y > 0$.  

The inflated beta regressions can be seen as hurdle models, as the log likelihood for the zero-and-one inflated beta regression can also be separated in two parts: First, a multinomial logistic regression modeling the probabilities of observing a $y = 0$, $y = 1$, or $0 < y < 1$; second, a beta regression model using just the cases where $0 < y < 1$. Similarly, the inflated beta regression at *either* zero *or* one has a log likelihood function that can be separated in two parts: First, a logistic regression modeling the probability of observing $y = c$ or $0 < y < 1$; second, a beta regression modeling just the cases where $0 < y < 1$ [@rigby2017distributions].  

Since their likelihoods can be separated, this implies an assumption: The two processes are independent, given the observed predictors. In other words, the error terms of each submodel are assumed to be independent of one another (where submodels refer to models predicting each parameter, $\mu$, $\sigma$, $\nu$, or $\tau$, separately). Dependencies between them can be modeled [referred to as "selection" models; @carlevaro2016multiple; @wooldridge2010econometric], but I will not discuss these models here, as they are not extended to beta regression.  

# Norms Produce Invariance, Ceiling and Floor Effects

@louis2003reflections argue that norms produce invariance. I empirically demonstrate the relationship between normative strength and invariance of attitudes, replicating the methodology used by @crandall2002social. In their Study 1, the authors presented participants with 105 social groups (e.g., Black people, librarians, deaf people, ugly people, Nazis, drug dealers). Participants were assigned to one of two groups: Half of the participants indicated how negatively they felt toward the group (i.e., self-reported prejudice), while the other half indicated how socially acceptable they perceived it was to feel and express negative feelings toward that group (i.e., acceptability of prejudice). Crandall and colleagues averaged the prejudice and acceptability scores at the group level, resulting in an $N = 105$ and two variables for each group: prejudice and acceptability. They found that expressed prejudice and acceptability of prejudice correlated at $r = .96$; people report prejudices that are socially acceptable [see @cary2014prevalence for a replication].  

I recruited 405 people from Amazon's Mechanical Turk website and randomly assigned them to either a *prejudice* or *acceptability* condition. All participants answered questions about 30 groups, randomly sampled from a pool of 120 groups. In the prejudice condition, participants were asked "How do you feel toward each of these groups?" and to indicate how they feel on a scale from "very negatively (0) to very positively (100)." I reverse-scored these items so that higher scores indicate more prejudice. In the acceptability condition, participants were asked "How OK is it in society for people to feel and say negative things about each of these groups?" and to indicate their perception on a scale from "totally unacceptable (0) to completely acceptable (100) for people to feel and say negative things about these groups."  

To demonstrate the relationship between norms and invariance, I calculated the median social acceptability score and the variance of the self-reported prejudice scores for each of the 120 groups. There was no linear relationship between social acceptability and variance of reported prejudice, $b = 0.31, SE = 0.84, t(118) = 0.37, p = .714$; however, there was a quadratic relationship regressing variance of reported prejudice on social acceptability, $\Delta R^2 = .28, b = -1250.07, SE = 187.44, t(117) = -6.67, p < .001$ (Figure 2). The more participants perceived prejudice to be influenced by norms—either being totally unacceptable (0) or completely acceptable (100)—the lower was the variance of self-reported prejudice by other participants.  

```{r echo = FALSE, fig.cap = "The stronger the normative influence on prejudice, the smaller the variance of reported prejudice."}
rawdat <- read_csv("../data/variance_norm.csv")
medians <- rawdat %>% 
  summarise_all(funs(median(., na.rm = TRUE))) %>%
  gather(var, value) %>% 
  separate(var, c("var", "group")) %>% 
  spread(var, value) %>% 
  transmute(group = group, accept_m = accept, prej_m = (100 - prej))
variances <- rawdat %>% 
  summarise_all(funs(var(., na.rm = TRUE))) %>%
  gather(var, value) %>% 
  separate(var, c("var", "group")) %>% 
  spread(var, value) %>% 
  transmute(group = group, accept_var = accept, prej_var = prej)
dat <- full_join(medians, variances)
ggplot(dat, aes(x = accept_m, y = prej_var)) + 
  geom_point(shape = 1) +
  geom_smooth(method = "lm", formula = y ~ poly(x, 2), 
              se = FALSE, color = "black") +
  theme_minimal() +
  labs(x = "Social Acceptability of Prejudice (Median)", 
       y = "Reported Prejudice (Variance)") +
  theme(text = element_text(size = 16))
```

I also investigated normative influences on floor effects. @crandall1994prejudice proposed a measure for anti-fat prejudice; in Study 5 of this paper, Crandall makes the argument that anti-fat attitudes are more socially acceptable (and less subject to social desirability biases) than anti-Black prejudice. He does this by demonstrating the decision-making process described above in the hurdle model: How often do people *always* select the least prejudiced option? That is, how often do people *always* respond at the lower bound of the distribution? He calls the proportion of people at the floor of a scale the "politically correct (PC) index," as this shows how often people respond in the most socially acceptable (i.e., politically correct) way possible. He finds that his anti-fat scale is less subject to these floor effects: 10% of participants always responded at the lower bound for anti-Black prejudice; this was only 3% for the anti-fat scale.  

To show that norms produce floor effects, I calculated Crandall's PC index for each of the 120 groups by counting how many people responded with a 0 on the reverse-scored prejudice scale (i.e., "totally positive" feelings). Using a negative binomial regression [@venables2002modern] for these count data, I regressed the PC index on social acceptability of the prejudice. Perceived social acceptability of the prejudice by one group of participants negatively predicted the PC index, calculated using a *separate group* of participants, $b = -0.03, SE = 0.002, Z = -10.97, p < .001$ (Figure 3). The more socially acceptable a prejudice, the less people opt for the "politically correct" floor of the scale.  

```{r echo = FALSE, fig.cap = "The more socially acceptable the prejudice, the less people opt for the politically correct response."}
pcdat <- rawdat %>% 
  select(starts_with("prej_")) %>% 
  summarise_all(funs(sum(. == 100, na.rm = TRUE))) %>% 
  gather(var, value) %>% 
  transmute(group = gsub("[^0-9]", "", var), pc_index = value) %>% 
  full_join(medians[, -3], by = "group")
pcfit <- MASS::glm.nb(pc_index ~ accept_m, pcdat)
fake_x <- seq(0, 100, .25)
pcpred <- predict(pcfit, data.frame(accept_m = fake_x), type = "response")
ggplot() +
  theme_minimal() +
  geom_point(aes(x = pcdat$accept_m, y = pcdat$pc_index), shape = 1) +
  geom_line(aes(x = fake_x, y = pcpred), size = 1) +
  labs(x = "Social Acceptability of Prejudice (Median)", y = "PC Index") +
  theme(text = element_text(size = 16))
```

This evidence shows that norms do positively predict invariance; predicting the scale parameter, $\sigma$, in beta regression could be used to investigate how much people adhere to norms regarding the expression of certain attitudes. These data also show that normative influence can lead to floor effects (or ceiling effects); predicting the shape parameters, $\nu$ and $\tau$, in beta regression could also be used to examine how much people adhere to norms. I now turn to demonstrating how these beta regression models can be used in R [@rcore2017] via the `gamlss` package [@rigby2005generalized].  

# Implementation in GAMLSS

```{r message = FALSE, warning = FALSE, error = FALSE, echo = FALSE}
library(gamlss)
```

The `gamlss` name stands for generalized additive models for location (i.e., $\mu$), scale (i.e., $\sigma$), and shape (i.e., $\nu$ and $\tau$). This package gives researchers the ability to use a wide variety of models, including beta regression models. A number of algorithms to estimate the coefficients can be used, described in detail by @stasinopoulos2017flexible. To first demonstrate how the package works according to the parameterization discussed in the Statistical Background section, I will generate a zero-and-one inflated beta distribution and estimate the parameters with an intercepts-only regression using the `gamlss` function. Then, I will demonstrate how to model social attitudes with real data.  

## Intercepts-Only Regression

We can set a number of parameters for this zero-and-one inflated distribution using the following code:  

```{r echo = TRUE}
n <- 5000
mu <- 0.40
sigma <- 0.60
p0 <- 0.13
p1 <- 0.17
p2 <- 1 - p0 - p1
a <- mu * (1 - sigma ^ 2) / (sigma ^ 2)
b <- a * (1 - mu) / mu
```

The sample size `n` $= 5000$, the mean `mu` $= 0.40$, the scale parameter `sigma` $= 0.60$, the probability of being 0 is `p0` $= 0.13$, the probability of being 1 is `p1` $= 0.17$, and the probability of being between 0 and 1 is `p2` $= 1 - p_0 - p_1 = .70$. Using the equations in the Statistical Background section, we can convert `mu` and `sigma` back to the original shape parameters `a` ($\alpha$) and `b` ($\beta$). The dependent variable `y` can now be generated using the following code:  

```{r echo = TRUE}
set.seed(1839)
y <- rbeta(n, a, b)
cat <- sample(1:3, n, prob = c(p0, p2, p1), replace = TRUE)
y[cat == 1] <- 0
y[cat == 3] <- 1
```

I draw `n` random numbers from the beta distribution with shape parameters `a` and `b`. I then draw `n` numbers ranging from 1 to 3 with probabilities `p0`, `p2`, and `p1`, respectively. If 1 was drawn, that observation is 0; if 2 was drawn, it remains being from the beta distribution; if 3 was drawn, that observation is 1. In this way, `p0`, `p2`, and `p1` determine the probability of being 0, from a beta distribution, or being 1, respectively.  

The `gamlss` function has four arguments for formulas—one for each of the parameters. It also has an argument, `family`, that determines what distribution will be used in fitting the model. For a zero-and-one inflated beta regression, the family is `BEINF()`. One can fit the intercepts-only model using the following code:  

```{r include = FALSE}
fit <- gamlss(
  formula = y ~ 1,
  formula.sigma = ~ 1,
  formula.nu = ~ 1,
  formula.tau = ~ 1,
  family = BEINF()
)
```
```{r eval = FALSE, echo = TRUE}
fit <- gamlss(
  formula = y ~ 1,     # formula for mu
  formula.sigma = ~ 1, # formula for sigma
  formula.nu = ~ 1,    # formula for nu
  formula.tau = ~ 1,   # formula for tau
  family = BEINF()     # distribution for model
)
```

One can use the inverse link functions to transform the coefficients back into the original scale; then, one can compare it to the parameters set above. The estimated parameters are calculated using the following code, according to the equations and link functions described in the Statistical Background section:  

```{r echo = TRUE}
inv_logit <- function(x) exp(x) / (1 + exp(x)) # inverse of link function
fit_mu <- inv_logit(fit$mu.coefficients)
fit_sigma <- inv_logit(fit$sigma.coefficients)
fit_nu <- exp(fit$nu.coefficients)
fit_tau <- exp(fit$tau.coefficients)
fit_p0 <- fit_nu / (1 + fit_nu + fit_tau)
fit_p1 <- fit_tau / (1 + fit_nu + fit_tau)
```

The estimates for $\mu$, $\sigma$, $p_0$, and $p_1$ are $.406$, $.597$, $.136$, and $.171$, respectively. These are close estimates to the population parameters set above. In research settings, each of the parameters would not be set to one number, but be regressed on observed variables that a researcher thinks predicts their values. For an exmample, I now turn to a model using real data.  

## Modeling Political Ideology

The social dominance orientation (SDO) scale measures how much one supports social hierarchy and inequality in society [@ho2015nature; @pratto1994social]. In personality psychology, it has a robust, oft-replicated positive relationship with political conservatism [@duriez2002march; @hiel2002explaining; @ho2015nature; @pratto2000social; @pratto1998social; @pratto1997gender; @wilson2013social]; some consider SDO a central aspect of contemporary American conservatism [@eidelman2012low; @jost2003political]. However, openly admitting that some groups are inherently better than others is a socially unacceptable thing to express, often leading to floor effects. I regress SDO on conservatism in both an OLS and beta regression framework to compare and contrast the two methods.  

The data come from Study 1 of @white2017freedom, where 175 participants answered an 8-item, 7-point SDO scale [@ho2015nature] and two 7-point items that assessed conservatism (i.e., Democrat, 1, to Republican, 7, and liberal, 1, to conservative, 7). Scales were calculated by averaging their respective items. Figure 4 shows the histogram of the SDO scale—30% of the sample responded with 1s for each of the 8 items, demonstrating a floor effect.  

```{r message = FALSE, warning = FALSE, fig.cap = "The distribution of SDO scores contains a floor effect."}
library(ggExtra)
library(gridExtra)
dat <- read_csv("../data/fors_study1.csv")
ggplot(dat, aes(x = sdo)) +
  theme_minimal() +
  geom_histogram(bins = 30) +
  labs(x = "Social Dominance Orientation", y = "Count") +
  theme(text = element_text(size = 16))
```

The two variables correlated at $r = .50, p < .001$. Figure 5 shows the predicted values against the standardized residuals from regressing SDO on conservatism using OLS, with a marginal histogram showing the distribution of residuals. A loess line is drawn, and a slight jitter has been added to the points, given that many points overlapped with one another. One can see a spread of residuals getting wider as predicted values increases, suggesting heteroskedasticity; moreover, the histogram shows that the residuals are positively skewed, violating the assumption of conditional normality. How does this look using beta regression?  

```{r message = FALSE, warning = FALSE, fig.cap = "The residuals plot shows heteroskedasticity and conditional non-normality."}
fit_ols_yhat <- lm(sdo ~ rw_polid, dat)$fitted.values
yhat_stdres <- ggplot(mapping = aes(
  x = fit_ols_yhat, y = rstandard(lm(sdo ~ rw_polid, dat))
  )) +
  theme_minimal() +
  geom_jitter(shape = 1, width = .1, height = .1) +
  geom_smooth(method = "loess", se = FALSE, color = "black") +
  labs(x = "Predicted Values",
       y = "Standardized Residuals") +
  theme(text = element_text(size = 16))
ggMarginal(yhat_stdres, type = "histogram", margins = "y")
```

First, I need to rescale the data. SDO was measured on a 1-to-7 scale, so $l = 1$ and $u = 7$. One can use the following R code to rescale a variable to $0 \leq y \leq 1$, following the equation described in the Modeling Bounded Variables Beyond the Zero Through One Range section:  

```{r echo = TRUE}
dat$sdo <- (dat$sdo - 1) / (7 - 1)
```

It is important to remember that $l$ and $u$ are *not* the minimum and maximum *observed* values, but the minimum and maximum *possible* values; for example, if lowest SDO score I observed was 2, $l$ would still be 1.  

```{r results = "asis"}
table1 <- data.frame(
  zeros <- c("No", "Yes", "No", "Yes"),
  ones <- c("No", "No", "Yes", "Yes"),
  dist <- c("Beta", "Zero-Inflated Beta", "One-Inflated Beta", 
            "Zero-and-One Inflated Beta"),
  family <- c("BE()", "BEINF0()", "BEINF1()", "BEINF()"),
  submodels <- c("$\\mu$, $\\sigma$", "$\\mu$, $\\sigma$, $\\nu$", 
                 "$\\mu$, $\\sigma$, $\\nu$", 
                 "$\\mu$, $\\sigma$, $\\nu$, $\\tau$")
)
papaja::apa_table(
  table1, escape = FALSE,
  col.names = c("Any $y_i = 0$?", "Any $y_i = 1$?", "Distribution", 
                "gamlss Family", "Submodels"),
  caption = "Guide for Choosing Correct Distribution and Corresponding gamlss Family"
)

```

The last thing a researcher has to do is decide which value to use for the `family` argument in the `gamlss` function. Researchers can follow Table 1 for deciding which value is appropriate. One can check to see the minimum and maximum observed by using the `range()` function in R. In this case, `range(dat$sdo)` returns `0 1`, showing that both 0s and 1s are observed. According to Table 1, I should use `BEINF()` for the `family`. I run the beta regression model using the following code:  

```{r include = FALSE}
fit_beinf <- gamlss(
  sdo ~ rw_polid,
  ~ rw_polid,
  ~ rw_polid,
  ~ rw_polid,
  data = dat,
  family = BEINF()
)
```
```{r eval = FALSE, echo = TRUE}
fit_beinf <- gamlss(sdo ~ rw_polid, ~ rw_polid, ~ rw_polid, ~ rw_polid,
                    data = dat, family = BEINF())
```

Finally, one can use the `summary()` function to examine the results. Table 2 shows the coefficients tables for each of the four submodels from the summary. The $\mu$ submodel tells the same story as the OLS model: The more conservative one is, the more they support hierarchy in society $b = 0.28, SE = 0.05, t(167) = 5.46, p < .001$. Since a logistic link function is used, however, this relationship is not linear (see Figure 7). The $\sigma$ submodel shows that, as people get more conservative, they also display more variance in their responses to the SDO scale, $b = 0.14, SE = 0.06, t(167) = 2.48, p = .014$; conservatives are not just more supportive of hierarchy, they also display a wider range of attitudes toward hierarchy. In line with the evidence presented above that norms produce invariance, one explanation for this relationship is that conservatives are less influenced by norms against expressing attitudes in favor of social inequality. The conservatism coefficient for the $\nu$ model is also significant: The more conservatism one reports, the less likely they are to respond at the floor [i.e., in a "politically correct" manner, @crandall1994prejudice], $b = -0.54, SE = 0.12, t(167) = -4.46, p < .001$. Again, an interpretation here is that conservatives are less influenced by the normative pressure to completely reject social inequality and group hierarchies. Lastly, the coefficient for the $\tau$ submodel is not significant; however, this could be an issue of statistical power, given that only one participant responded at the ceiling.  

```{r results = "asis"}
table2 <- data.frame(
  Submodel = c("$\\mu$", "", "$\\sigma$", "", "$\\nu$", "", "$\\tau$", ""),
  Variable = rep(c("Intercept", "Conservatism"), 4),
  b = c(-2.03, 0.28, -0.97, 0.14, 0.82, -0.54, -13.33, 1.70),
  SE = c(0.20, 0.05, 0.22, 0.06, 0.38, 0.12, 6.91, 1.15),
  t = c(-10.15, 5.46, -4.42, 2.48, 2.13, -4.46, -1.93, 1.47),
  p = c("< .001", "< .001", "< .001", ".014", ".034", "< .001", ".055", ".143")
)
papaja::apa_table(
  table2, escape = FALSE, align = c("l", "l", "r", "r", "r", "r"),
  col.names = c("Submodel", "Variable", "$b$", "$SE$", "$t$", "$p$"),
  caption = "Results From the Beta Regression Model"
)
```

How do the OLS and beta regression results compare? Figure 6 shows a scatterplot of SDO regressed onto conservatism using OLS regression. The solid line is the regression line. The estimated standard deviation of the residuals, $\sigma_\epsilon$, is added to and subtracted from this line; these are the dotted lines. In line with the assumption of homoskedasticity, this standard deviation is constant across all levels of conservatism. Importantly, the lower dotted line dips below observed values—the model assumes that predicted values come from a distribution that has values in it that *cannot* be observed with the Likert scale used.  

```{r message = FALSE, warning = FALSE, fig.cap = "SDO regressed on conservatism, using OLS regression. Dotted lines represent are one standard deviation (i.e., estimated standard deviation of the residuals) above and below the solid regression line."}
fit_ols <- lm(sdo ~ rw_polid, dat)
ggplot(dat, aes(x = rw_polid, y = sdo)) +
  theme_minimal() +
  geom_jitter(shape = 1) +
  geom_smooth(method = "lm", se = FALSE, color = "black") +
  geom_line(aes(y = fit_ols$fitted.values + summary(fit_ols)$sigma), 
            size = .75, linetype = 2) +
  geom_line(aes(y = fit_ols$fitted.values - summary(fit_ols)$sigma), 
            size = .75, linetype = 2) +
  labs(x = "Conservatism", 
       y = "Social Dominance Orientation") +
  theme(text = element_text(size = 16))
```

Figure 7 shows the same figure for the zero-and-one inflated beta regression model. The width of the dotted line increases as conservatism increases, demonstrating the heteroskedasticity of the model. This also shows that variance in SDO scores increases with conservatism; also note that the dotted line does not dip below the floor, as values in the beta distribution cannot be less than zero.  

```{r message = FALSE, warning = FALSE, fig.cap = "SDO regressed on conservatism, using beta regression. Dotted lines represent are one standard deviation (i.e., estimated standard deviation of the residuals) above and below the solid regression line."}
b_sig <- sqrt(fit_beinf$sigma.fv ^ 2 * fit_beinf$mu.fv * (1 - fit_beinf$mu.fv))
ggplot(dat, aes(x = rw_polid, y = sdo)) +
  theme_minimal() +
  geom_jitter(shape = 1) +
  geom_line(aes(y = fit_beinf$mu.fv), 
            size = 1.0, linetype = 1) +
  geom_line(aes(y = fit_beinf$mu.fv + b_sig), 
            size = .75, linetype = 2) +
  geom_line(aes(y = fit_beinf$mu.fv - b_sig), 
            size = .75, linetype = 2) +
  labs(x = "Conservatism", 
       y = "Social Dominance Orientation") +
  theme(text = element_text(size = 16))
```

# Discussion

\newpage

# References
```{r create_r-references}
papaja::r_refs(file = "r-references.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
